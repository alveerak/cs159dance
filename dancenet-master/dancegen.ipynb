{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/brandonquach/anaconda/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 120, 208, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 120, 208, 128 1280        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 60, 104, 128) 0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 60, 104, 64)  73792       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 30, 52, 64)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 30, 52, 32)   18464       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 15, 26, 32)   0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 12480)        0           max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          1597568     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          16512       dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          16512       dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z (Lambda)                      (None, 128)          0           dense_2[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,724,128\n",
      "Trainable params: 1,724,128\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "z_sampling (InputLayer)      (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 12480)             1609920   \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 15, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 15, 26, 128)       4224      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 15, 26, 32)        36896     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 30, 52, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 30, 52, 64)        18496     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 60, 104, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 60, 104, 128)      73856     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 120, 208, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 120, 208, 1)       1153      \n",
      "=================================================================\n",
      "Total params: 1,744,545\n",
      "Trainable params: 1,744,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(?, 120, 208, 1)\n"
     ]
    }
   ],
   "source": [
    "from model import vae,decoder\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.optimizers import adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mdn\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_generator(batch_size,limit):\n",
    "\n",
    "\tbatch = []\n",
    "\tcounter = 1\n",
    "\twhile 1:\n",
    "\t\tfor i in range(1,limit+1):\n",
    "\t\t\tif counter >= limit:\n",
    "\t\t\t\tcounter = 1\n",
    "\t\t\timg = cv2.imread(\"imgs/{}.jpg\".format(counter),cv2.IMREAD_GRAYSCALE)\n",
    "\t\t\timg = img.reshape(120,208,1)\n",
    "\t\t\tbatch.append(img)\n",
    "\t\t\tif len(batch) == batch_size:\n",
    "\t\t\t\tbatch_np = np.array(batch) / 255\n",
    "\t\t\t\tbatch = []\n",
    "\t\t\t\treturn batch_np\n",
    "\t\t\tcounter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "X = data_generator(batch_size = 47950, limit = len(os.listdir('imgs')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gumbel Softmax VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandonquach/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:72: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 111s 15ms/step - loss: 5900.9438 - val_loss: 4198.8210\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 111s 15ms/step - loss: 4909.9179 - val_loss: 4176.0520\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 99s 13ms/step - loss: 4880.8507 - val_loss: 4172.3943\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 113s 15ms/step - loss: 4859.0903 - val_loss: 4140.7443\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 117s 16ms/step - loss: 4847.2770 - val_loss: 4147.5519\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 114s 15ms/step - loss: 4857.0861 - val_loss: 4155.5361\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 110s 15ms/step - loss: 4846.1662 - val_loss: 4134.8994\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 106s 14ms/step - loss: 4830.0474 - val_loss: 4128.1299\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 107s 14ms/step - loss: 4848.3387 - val_loss: 4114.0088\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 111s 15ms/step - loss: 4843.0759 - val_loss: 4117.1617\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 112s 15ms/step - loss: 4826.3248 - val_loss: 4159.2050\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 110s 15ms/step - loss: 4849.2739 - val_loss: 4111.6728\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 100s 13ms/step - loss: 4841.9158 - val_loss: 4144.6569\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 100s 13ms/step - loss: 4837.2299 - val_loss: 4139.0924\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 101s 14ms/step - loss: 4819.9295 - val_loss: 4127.1476\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 103s 14ms/step - loss: 4816.9014 - val_loss: 4111.0037\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 111s 15ms/step - loss: 4805.8687 - val_loss: 4126.0287\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 134s 18ms/step - loss: 4817.4010 - val_loss: 4098.8472\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 107s 14ms/step - loss: 4807.2379 - val_loss: 4095.0799\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 109s 15ms/step - loss: 4801.1759 - val_loss: 4149.8584\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 133s 18ms/step - loss: 4787.2057 - val_loss: 4121.1415\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 121s 16ms/step - loss: 4797.3643 - val_loss: 4096.8723\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 112s 15ms/step - loss: 4801.5583 - val_loss: 4084.3480\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 116s 15ms/step - loss: 4790.0988 - val_loss: 4086.7974\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 123s 16ms/step - loss: 4791.4264 - val_loss: 4078.1921\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 118s 16ms/step - loss: 4780.4343 - val_loss: 4076.8872\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 116s 15ms/step - loss: 4777.3074 - val_loss: 4082.6776\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 111s 15ms/step - loss: 4780.9999 - val_loss: 4076.6710\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 112s 15ms/step - loss: 4771.5246 - val_loss: 4076.3972\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 114s 15ms/step - loss: 4773.2646 - val_loss: 4075.4892\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 113s 15ms/step - loss: 4767.4074 - val_loss: 4071.1104\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 114s 15ms/step - loss: 4764.8834 - val_loss: 4071.7717\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 110s 15ms/step - loss: 4765.7001 - val_loss: 4067.2084\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 109s 15ms/step - loss: 4758.8125 - val_loss: 4090.0802\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 111s 15ms/step - loss: 4780.8722 - val_loss: 4068.3777\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 102s 14ms/step - loss: 4756.8182 - val_loss: 4069.6000\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 98s 13ms/step - loss: 4756.6074 - val_loss: 4066.9479\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 95s 13ms/step - loss: 4752.4724 - val_loss: 4066.4569\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 94s 12ms/step - loss: 4750.9502 - val_loss: 4065.5038\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 93s 12ms/step - loss: 4749.2155 - val_loss: 4064.8463\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 95s 13ms/step - loss: 4747.7566 - val_loss: 4064.2253\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 104s 14ms/step - loss: 4746.4432 - val_loss: 4063.7002\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 97s 13ms/step - loss: 4745.1239 - val_loss: 4063.2336\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 107s 14ms/step - loss: 4744.0572 - val_loss: 4062.6714\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 96s 13ms/step - loss: 4743.0270 - val_loss: 4062.2483\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 97s 13ms/step - loss: 4740.4671 - val_loss: 4065.5495\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 96s 13ms/step - loss: 4746.2810 - val_loss: 4061.6862\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 96s 13ms/step - loss: 4740.1508 - val_loss: 4061.1911\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 102s 14ms/step - loss: 4738.9994 - val_loss: 4060.8564\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 97s 13ms/step - loss: 4738.0570 - val_loss: 4060.4987\n",
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "7400/7500 [============================>.] - ETA: 1s - loss: 4746.5867"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras.models import Model, Sequential\n",
    "from keras import backend as K\n",
    "from keras import objectives\n",
    "from keras.datasets import mnist\n",
    "from keras.activations import softmax\n",
    "from keras.objectives import binary_crossentropy as bce\n",
    "\n",
    "batch_size = 100\n",
    "data_dim = 24960\n",
    "M = 80\n",
    "N = 240\n",
    "\n",
    "nb_epoch = 100\n",
    "epsilon_std = 0.01\n",
    "\n",
    "anneal_rate = 0.0003\n",
    "min_temperature = 0.5\n",
    "\n",
    "tau = K.variable(5.0, name=\"temperature\")\n",
    "x = Input(shape = (data_dim,))\n",
    "h = Dense(256, activation='relu')(Dense(512, activation='relu')(x))\n",
    "logits_y = Dense(M*N)(h)\n",
    "\n",
    "def sampling(logits_y):\n",
    "    U = K.random_uniform(K.shape(logits_y), 0, 1)\n",
    "    y = logits_y - K.log(-K.log(U + 1e-20) + 1e-20) # logits + gumbel noise\n",
    "    y = softmax(K.reshape(y, (-1, N, M)) / tau)\n",
    "    y = K.reshape(y, (-1, N*M))\n",
    "    return y\n",
    "\n",
    "z = Lambda(sampling, output_shape=(M*N,))(logits_y)\n",
    "generator = Sequential()\n",
    "generator.add(Dense(256, activation='relu', input_shape=(N*M, )))\n",
    "generator.add(Dense(512, activation='relu'))\n",
    "generator.add(Dense(data_dim, activation='sigmoid'))\n",
    "x_hat = generator(z)\n",
    "\n",
    "# x_hat = Dense(data_dim, activation='softmax')(Dense(512, activation='relu')(Dense(256, activation='relu')(z)))\n",
    "def gumbel_loss(x, x_hat):\n",
    "    q_y = K.reshape(logits_y, (-1, N, M))\n",
    "    q_y = softmax(q_y)\n",
    "    log_q_y = K.log(q_y + 1e-20)\n",
    "    kl_tmp = q_y * (log_q_y - K.log(1.0/M))\n",
    "    KL = K.sum(kl_tmp, axis=(1, 2))\n",
    "    elbo = data_dim * bce(x, x_hat) - KL \n",
    "    return elbo\n",
    "\n",
    "model = Model(x, x_hat)\n",
    "model.compile(optimizer='adam', loss=gumbel_loss)\n",
    "\n",
    "X = data[0:len(data)-1]\n",
    "X = X.reshape((len(X), np.prod(X.shape[1:])))\n",
    "y = data[1:len(data)]\n",
    "y = y.reshape((len(y), np.prod(y.shape[1:])))\n",
    "\n",
    "y = y[:47900]\n",
    "y_train = y[:38300]\n",
    "y_test = y[38300:]\n",
    "X = X[:47900]\n",
    "X_train = X[:38300]\n",
    "X_test = X[38300:]\n",
    "\n",
    "for e in range(nb_epoch):\n",
    "    model.fit(X_train, y_train,\n",
    "        shuffle=False,\n",
    "        nb_epoch=1,\n",
    "        batch_size=batch_size,\n",
    "        validation_data = (X_test, y_test))\n",
    "    K.set_value(tau, np.max([K.get_value(tau) * np.exp(- anneal_rate * e), min_temperature]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9993643  0.9992611  0.9993783  ... 0.99631715 0.99758744 0.99768126]]\n",
      "[[0.9993648 0.9992617 0.9993789 ... 0.9963194 0.9975891 0.9976828]]\n",
      "[[0.9993656  0.99926263 0.99937975 ... 0.996323   0.9975915  0.9976852 ]]\n",
      "[[0.99936515 0.99926215 0.9993793  ... 0.99632096 0.99759007 0.9976839 ]]\n",
      "[[0.99936575 0.99926275 0.99937975 ... 0.9963231  0.9975916  0.9976853 ]]\n",
      "[[0.9993654  0.9992623  0.9993794  ... 0.99632156 0.99759054 0.99768424]]\n",
      "[[0.99936527 0.99926215 0.9993793  ... 0.9963212  0.9975903  0.997684  ]]\n",
      "[[0.99936396 0.9992607  0.9993781  ... 0.9963158  0.9975865  0.9976803 ]]\n",
      "[[0.99936455 0.9992613  0.99937856 ... 0.996318   0.99758804 0.99768186]]\n",
      "[[0.9993648  0.9992617  0.9993789  ... 0.99631953 0.9975891  0.9976828 ]]\n",
      "[[0.99936503 0.9992619  0.99937904 ... 0.99632025 0.9975896  0.9976834 ]]\n",
      "[[0.99936587 0.99926287 0.9993799  ... 0.9963238  0.9975922  0.9976858 ]]\n",
      "[[0.9993648  0.99926156 0.9993788  ... 0.99631906 0.9975889  0.9976826 ]]\n",
      "[[0.9993636  0.99926025 0.9993777  ... 0.9963141  0.9975853  0.9976793 ]]\n",
      "[[0.9993649  0.9992618  0.99937904 ... 0.9963199  0.99758935 0.99768317]]\n",
      "[[0.99936455 0.9992613  0.9993787  ... 0.9963182  0.9975883  0.997682  ]]\n",
      "[[0.99936515 0.99926203 0.99937916 ... 0.9963207  0.99759007 0.99768376]]\n",
      "[[0.99936527 0.99926215 0.9993793  ... 0.9963211  0.9975902  0.9976839 ]]\n",
      "[[0.99936503 0.9992619  0.99937904 ... 0.99632025 0.9975896  0.9976834 ]]\n",
      "[[0.9993655 0.9992625 0.9993795 ... 0.9963223 0.997591  0.9976847]]\n",
      "[[0.99936503 0.9992619  0.99937904 ... 0.99632025 0.9975896  0.9976834 ]]\n",
      "[[0.99936515 0.99926203 0.99937916 ... 0.9963206  0.9975898  0.99768364]]\n",
      "[[0.99936503 0.9992619  0.99937904 ... 0.99632    0.99758947 0.9976833 ]]\n",
      "[[0.99936503 0.9992619  0.99937904 ... 0.99632025 0.9975896  0.9976834 ]]\n",
      "[[0.99936515 0.99926203 0.99937916 ... 0.9963206  0.99758995 0.99768364]]\n",
      "[[0.99936527 0.9992623  0.9993793  ... 0.9963213  0.9975904  0.9976841 ]]\n",
      "[[0.9993655  0.9992625  0.9993795  ... 0.9963223  0.99759114 0.99768484]]\n",
      "[[0.9993648 0.9992617 0.9993789 ... 0.9963193 0.997589  0.9976827]]\n",
      "[[0.9993648 0.9992617 0.9993788 ... 0.9963192 0.997589  0.9976827]]\n",
      "[[0.9993655  0.9992625  0.99937963 ... 0.9963224  0.99759114 0.99768484]]\n",
      "[[0.9993648  0.9992617  0.9993789  ... 0.99631953 0.9975891  0.9976828 ]]\n",
      "[[0.9993654  0.9992623  0.9993794  ... 0.9963217  0.99759066 0.99768436]]\n",
      "[[0.9993656  0.99926263 0.99937963 ... 0.99632263 0.9975914  0.99768496]]\n",
      "[[0.99936455 0.9992613  0.99937856 ... 0.9963181  0.99758816 0.997682  ]]\n",
      "[[0.99936503 0.9992619  0.99937916 ... 0.99632037 0.9975897  0.9976835 ]]\n",
      "[[0.9993649  0.9992618  0.9993789  ... 0.9963198  0.99758935 0.99768305]]\n",
      "[[0.99936503 0.9992619  0.99937916 ... 0.9963205  0.9975898  0.9976835 ]]\n",
      "[[0.99936527 0.99926215 0.9993793  ... 0.9963212  0.9975903  0.9976841 ]]\n",
      "[[0.9993647  0.99926156 0.9993788  ... 0.9963188  0.99758863 0.99768245]]\n",
      "[[0.9993655  0.9992625  0.9993795  ... 0.9963224  0.99759114 0.99768484]]\n",
      "[[0.9993656  0.9992625  0.99937963 ... 0.99632263 0.99759126 0.99768496]]\n",
      "[[0.9993655  0.9992624  0.9993795  ... 0.99632204 0.9975909  0.9976846 ]]\n",
      "[[0.99936503 0.9992619  0.99937916 ... 0.99632037 0.9975897  0.9976835 ]]\n",
      "[[0.9993654 0.9992624 0.9993794 ... 0.9963218 0.9975908 0.9976845]]\n",
      "[[0.99936503 0.9992619  0.99937904 ... 0.99632    0.9975896  0.9976833 ]]\n",
      "[[0.9993654  0.9992623  0.9993794  ... 0.99632156 0.99759066 0.99768436]]\n",
      "[[0.9993655 0.9992625 0.9993795 ... 0.9963223 0.997591  0.9976847]]\n",
      "[[0.99936515 0.99926203 0.99937916 ... 0.9963205  0.9975898  0.9976835 ]]\n",
      "[[0.9993656  0.9992625  0.99937963 ... 0.9963225  0.99759126 0.99768496]]\n",
      "[[0.99936527 0.9992623  0.9993793  ... 0.9963213  0.9975904  0.9976841 ]]\n",
      "[[0.99936527 0.99926215 0.9993793  ... 0.9963212  0.9975903  0.997684  ]]\n",
      "[[0.9993654  0.9992623  0.9993794  ... 0.9963217  0.99759066 0.99768436]]\n",
      "[[0.9993643  0.9992611  0.99937844 ... 0.99631727 0.99758756 0.9976814 ]]\n",
      "[[0.99936503 0.99926203 0.99937916 ... 0.9963205  0.9975898  0.9976835 ]]\n",
      "[[0.99936515 0.99926203 0.9993793  ... 0.99632084 0.99759007 0.99768376]]\n",
      "[[0.9993656  0.99926263 0.99937963 ... 0.99632263 0.9975914  0.9976851 ]]\n",
      "[[0.99936515 0.99926203 0.99937916 ... 0.9963206  0.9975898  0.99768364]]\n",
      "[[0.9993655  0.9992624  0.9993795  ... 0.99632204 0.9975909  0.9976846 ]]\n",
      "[[0.9993654 0.9992624 0.9993794 ... 0.9963218 0.9975908 0.9976845]]\n",
      "[[0.99936587 0.99926287 0.9993799  ... 0.9963237  0.9975921  0.9976858 ]]\n",
      "[[0.9993654  0.9992623  0.9993794  ... 0.9963217  0.99759066 0.99768436]]\n",
      "[[0.9993647  0.99926156 0.9993787  ... 0.9963187  0.99758863 0.99768233]]\n",
      "[[0.99936515 0.99926203 0.99937916 ... 0.9963205  0.9975898  0.9976835 ]]\n",
      "[[0.9993654  0.9992623  0.9993794  ... 0.99632156 0.99759054 0.99768436]]\n",
      "[[0.9993649  0.9992618  0.9993789  ... 0.9963198  0.99758935 0.99768305]]\n",
      "[[0.9993654 0.9992624 0.9993794 ... 0.9963218 0.9975908 0.9976845]]\n",
      "[[0.9993647  0.99926156 0.9993788  ... 0.9963188  0.99758863 0.99768245]]\n",
      "[[0.9993654  0.9992623  0.9993794  ... 0.99632156 0.99759054 0.99768424]]\n",
      "[[0.9993655  0.9992624  0.9993795  ... 0.99632204 0.9975909  0.9976846 ]]\n",
      "[[0.99936527 0.99926215 0.9993793  ... 0.9963211  0.9975903  0.997684  ]]\n",
      "[[0.99936384 0.9992606  0.99937797 ... 0.9963154  0.99758625 0.9976801 ]]\n",
      "[[0.9993656  0.9992625  0.99937963 ... 0.9963225  0.99759126 0.99768496]]\n",
      "[[0.9993654  0.9992623  0.9993794  ... 0.9963217  0.99759066 0.99768436]]\n",
      "[[0.9993649  0.9992617  0.9993789  ... 0.99631953 0.9975891  0.9976829 ]]\n",
      "[[0.999366   0.999263   0.99938    ... 0.99632406 0.9975923  0.997686  ]]\n",
      "[[0.99936515 0.99926215 0.9993793  ... 0.99632084 0.99759007 0.9976839 ]]\n",
      "[[0.99936527 0.99926215 0.9993793  ... 0.9963211  0.9975902  0.997684  ]]\n",
      "[[0.99936503 0.9992619  0.99937904 ... 0.9963201  0.9975896  0.9976833 ]]\n",
      "[[0.9993656  0.9992625  0.99937963 ... 0.99632263 0.99759126 0.99768496]]\n",
      "[[0.99936527 0.99926215 0.9993793  ... 0.9963211  0.9975902  0.9976839 ]]\n",
      "[[0.9993654  0.9992623  0.9993794  ... 0.99632156 0.99759054 0.99768436]]\n",
      "[[0.9993654  0.9992623  0.9993794  ... 0.99632144 0.99759054 0.99768424]]\n",
      "[[0.9993654  0.9992623  0.9993794  ... 0.9963217  0.99759066 0.99768436]]\n",
      "[[0.99936527 0.99926215 0.9993793  ... 0.9963212  0.9975903  0.997684  ]]\n",
      "[[0.9993654  0.9992623  0.9993794  ... 0.9963217  0.99759066 0.99768436]]\n",
      "[[0.99936503 0.9992619  0.99937916 ... 0.99632037 0.9975898  0.9976835 ]]\n",
      "[[0.9993656  0.9992625  0.99937963 ... 0.9963225  0.99759126 0.99768496]]\n",
      "[[0.99936503 0.99926203 0.99937916 ... 0.9963205  0.9975898  0.9976835 ]]\n",
      "[[0.9993655  0.9992625  0.99937963 ... 0.9963225  0.99759126 0.99768496]]\n",
      "[[0.9993656  0.99926263 0.99937963 ... 0.9963229  0.9975915  0.9976852 ]]\n",
      "[[0.9993647  0.99926156 0.9993788  ... 0.99631894 0.99758875 0.9976826 ]]\n",
      "[[0.99936503 0.99926203 0.99937916 ... 0.9963205  0.9975898  0.9976835 ]]\n",
      "[[0.99936527 0.9992623  0.9993794  ... 0.99632144 0.9975904  0.99768424]]\n",
      "[[0.99936515 0.99926203 0.99937916 ... 0.9963207  0.99758995 0.99768376]]\n",
      "[[0.99936503 0.9992619  0.99937904 ... 0.99632025 0.9975897  0.9976834 ]]\n",
      "[[0.99936527 0.99926215 0.9993793  ... 0.9963212  0.9975903  0.997684  ]]\n",
      "[[0.9993654 0.9992624 0.9993794 ... 0.9963218 0.9975908 0.9976845]]\n",
      "[[0.9993649  0.9992618  0.99937904 ... 0.9963199  0.99758935 0.99768317]]\n",
      "[[0.9993654  0.9992623  0.9993794  ... 0.9963217  0.99759066 0.99768436]]\n",
      "[[0.9993636  0.99926025 0.9993777  ... 0.99631435 0.99758554 0.9976794 ]]\n",
      "[[0.9993647  0.99926156 0.9993788  ... 0.9963188  0.99758863 0.99768245]]\n",
      "[[0.99936503 0.9992619  0.99937916 ... 0.99632037 0.9975897  0.9976835 ]]\n",
      "[[0.9993654 0.9992624 0.9993795 ... 0.9963219 0.9975908 0.9976845]]\n",
      "[[0.9993654 0.9992624 0.9993794 ... 0.9963218 0.9975908 0.9976845]]\n",
      "[[0.99936575 0.99926275 0.99937975 ... 0.99632335 0.99759185 0.99768555]]\n",
      "[[0.99936587 0.99926287 0.9993799  ... 0.99632347 0.997592   0.99768555]]\n",
      "[[0.9993656  0.99926263 0.99937963 ... 0.9963229  0.9975915  0.9976852 ]]\n",
      "[[0.9993654 0.9992624 0.9993795 ... 0.9963219 0.9975908 0.9976845]]\n",
      "[[0.9993654  0.9992623  0.9993794  ... 0.99632156 0.99759054 0.99768424]]\n",
      "[[0.99936396 0.9992607  0.9993781  ... 0.9963159  0.9975866  0.9976804 ]]\n",
      "[[0.9993655  0.9992625  0.9993795  ... 0.9963223  0.99759114 0.99768484]]\n",
      "[[0.9993648 0.9992617 0.9993789 ... 0.9963194 0.9975891 0.9976828]]\n",
      "[[0.9993655  0.9992624  0.9993795  ... 0.99632215 0.997591   0.9976847 ]]\n",
      "[[0.9993649  0.9992618  0.99937904 ... 0.9963199  0.99758935 0.99768317]]\n",
      "[[0.99936575 0.99926275 0.99937975 ... 0.9963232  0.99759173 0.99768543]]\n",
      "[[0.99936455 0.9992613  0.9993787  ... 0.9963182  0.9975883  0.997682  ]]\n",
      "[[0.99936575 0.99926263 0.99937975 ... 0.996323   0.9975916  0.9976853 ]]\n",
      "[[0.99936575 0.99926275 0.99937975 ... 0.9963231  0.9975916  0.9976853 ]]\n",
      "[[0.9993656  0.99926263 0.99937963 ... 0.99632275 0.9975914  0.9976852 ]]\n",
      "[[0.9993649  0.9992618  0.9993789  ... 0.99631965 0.99758923 0.99768305]]\n",
      "[[0.99936515 0.99926203 0.99937916 ... 0.9963205  0.9975898  0.99768364]]\n",
      "[[0.9993654 0.9992624 0.9993794 ... 0.9963218 0.9975908 0.9976845]]\n",
      "[[0.9993649  0.9992618  0.99937904 ... 0.9963199  0.99758947 0.99768317]]\n",
      "[[0.99936515 0.99926203 0.99937916 ... 0.9963207  0.99758995 0.99768364]]\n",
      "[[0.9993648 0.9992617 0.9993788 ... 0.9963192 0.9975889 0.9976827]]\n",
      "[[0.9993654  0.9992623  0.9993794  ... 0.99632156 0.99759054 0.99768436]]\n",
      "[[0.9993655  0.9992625  0.99937963 ... 0.9963224  0.99759114 0.99768484]]\n",
      "[[0.99936527 0.9992623  0.9993793  ... 0.9963213  0.9975904  0.9976841 ]]\n",
      "[[0.99936515 0.99926203 0.99937916 ... 0.9963207  0.99759007 0.99768376]]\n",
      "[[0.99936503 0.9992619  0.99937916 ... 0.99632037 0.9975897  0.9976835 ]]\n",
      "[[0.9993649  0.9992618  0.9993789  ... 0.99631953 0.99758923 0.9976829 ]]\n",
      "[[0.99936527 0.99926215 0.9993793  ... 0.99632096 0.9975902  0.9976839 ]]\n",
      "[[0.9993656  0.9992625  0.99937963 ... 0.99632263 0.99759126 0.99768496]]\n",
      "[[0.9993647  0.99926156 0.9993788  ... 0.9963188  0.99758863 0.99768245]]\n",
      "[[0.9993641  0.99926084 0.9993782  ... 0.99631643 0.99758697 0.9976808 ]]\n",
      "[[0.9993643  0.9992612  0.99937844 ... 0.9963174  0.9975877  0.9976815 ]]\n",
      "[[0.9993647  0.99926144 0.9993787  ... 0.9963186  0.9975885  0.9976822 ]]\n",
      "[[0.9993655  0.9992625  0.99937963 ... 0.9963224  0.99759114 0.99768484]]\n",
      "[[0.9993656  0.99926263 0.99937963 ... 0.9963229  0.9975915  0.9976852 ]]\n",
      "[[0.99936515 0.99926203 0.99937916 ... 0.99632084 0.99759007 0.99768376]]\n",
      "[[0.99936527 0.99926215 0.9993793  ... 0.9963211  0.9975903  0.997684  ]]\n",
      "[[0.9993655  0.9992625  0.9993795  ... 0.9963223  0.99759114 0.99768484]]\n",
      "[[0.99936527 0.9992623  0.9993794  ... 0.99632144 0.9975904  0.99768424]]\n",
      "[[0.99936515 0.99926203 0.99937916 ... 0.9963206  0.99758995 0.99768364]]\n",
      "[[0.99936503 0.9992619  0.99937904 ... 0.99632025 0.9975896  0.9976834 ]]\n",
      "[[0.9993655  0.9992625  0.9993795  ... 0.99632215 0.997591   0.9976847 ]]\n",
      "[[0.99936134 0.99925774 0.99937564 ... 0.9963052  0.99757916 0.9976731 ]]\n",
      "[[0.9993655  0.9992624  0.9993795  ... 0.99632215 0.997591   0.9976847 ]]\n",
      "[[0.9993654 0.9992624 0.9993795 ... 0.9963219 0.9975909 0.9976846]]\n",
      "[[0.99936527 0.99926215 0.9993793  ... 0.9963211  0.9975903  0.997684  ]]\n",
      "[[0.99936575 0.99926263 0.99937975 ... 0.996323   0.9975916  0.9976853 ]]\n",
      "[[0.9993641  0.99926084 0.9993782  ... 0.99631643 0.99758697 0.9976808 ]]\n",
      "[[0.99936384 0.9992605  0.99937797 ... 0.9963152  0.997586   0.99768   ]]\n",
      "[[0.99936515 0.99926203 0.99937916 ... 0.9963207  0.99759007 0.99768376]]\n",
      "[[0.9993656  0.99926263 0.99937963 ... 0.99632275 0.9975914  0.9976851 ]]\n",
      "[[0.99936515 0.99926203 0.99937916 ... 0.99632084 0.99759007 0.99768376]]\n",
      "[[0.99936527 0.9992623  0.9993794  ... 0.99632144 0.99759054 0.99768424]]\n",
      "[[0.9993648  0.99926156 0.9993788  ... 0.99631906 0.9975889  0.9976826 ]]\n",
      "[[0.9993647  0.99926156 0.9993788  ... 0.99631894 0.99758875 0.99768245]]\n",
      "[[0.9993656  0.99926263 0.99937963 ... 0.99632263 0.99759126 0.99768496]]\n",
      "[[0.99936515 0.99926203 0.99937916 ... 0.9963207  0.99758995 0.99768376]]\n",
      "[[0.9993656  0.9992625  0.99937963 ... 0.99632263 0.99759126 0.99768496]]\n",
      "[[0.9993654 0.9992624 0.9993794 ... 0.9963218 0.9975908 0.9976845]]\n",
      "[[0.99936515 0.99926203 0.99937916 ... 0.9963205  0.9975898  0.9976835 ]]\n",
      "[[0.99936515 0.99926203 0.99937916 ... 0.9963207  0.99759007 0.99768376]]\n",
      "[[0.9993655 0.9992625 0.9993795 ... 0.9963223 0.997591  0.9976847]]\n",
      "[[0.9993656  0.99926263 0.99937963 ... 0.99632263 0.9975914  0.9976851 ]]\n",
      "[[0.99936527 0.9992623  0.9993794  ... 0.9963213  0.9975904  0.9976841 ]]\n",
      "[[0.99936503 0.9992619  0.99937904 ... 0.99632025 0.9975897  0.9976834 ]]\n",
      "[[0.9993655  0.9992625  0.99937963 ... 0.9963224  0.99759114 0.99768484]]\n",
      "[[0.99936575 0.99926275 0.99937975 ... 0.99632335 0.99759185 0.99768555]]\n",
      "[[0.9993647  0.99926156 0.9993788  ... 0.99631894 0.99758875 0.9976826 ]]\n",
      "[[0.9993654  0.9992623  0.9993794  ... 0.9963217  0.99759066 0.99768436]]\n",
      "[[0.9993648 0.9992617 0.9993788 ... 0.9963192 0.997589  0.9976827]]\n",
      "[[0.99936503 0.9992619  0.99937904 ... 0.99632    0.99758947 0.9976833 ]]\n",
      "[[0.9993649  0.9992618  0.99937904 ... 0.9963199  0.99758935 0.99768317]]\n",
      "[[0.99936336 0.99926    0.99937755 ... 0.9963134  0.9975848  0.9976787 ]]\n",
      "[[0.9993654 0.9992624 0.9993794 ... 0.9963218 0.9975908 0.9976845]]\n",
      "[[0.99936527 0.99926215 0.9993793  ... 0.9963211  0.9975903  0.997684  ]]\n",
      "[[0.9993648  0.99926156 0.9993788  ... 0.99631906 0.9975889  0.9976826 ]]\n",
      "[[0.9993654 0.9992624 0.9993794 ... 0.9963218 0.9975908 0.9976845]]\n",
      "[[0.9993631  0.99925965 0.9993773  ... 0.9963122  0.997584   0.997678  ]]\n",
      "[[0.999366  0.999263  0.99938   ... 0.9963242 0.9975924 0.997686 ]]\n",
      "[[0.9993642  0.99926096 0.9993782  ... 0.99631655 0.9975871  0.9976809 ]]\n",
      "[[0.9993655  0.9992625  0.9993795  ... 0.9963224  0.99759114 0.99768484]]\n",
      "[[0.99936527 0.99926215 0.9993793  ... 0.9963211  0.9975903  0.997684  ]]\n",
      "[[0.99936444 0.9992612  0.99937844 ... 0.9963175  0.9975878  0.9976815 ]]\n",
      "[[0.9993655 0.9992625 0.9993795 ... 0.9963223 0.997591  0.9976847]]\n",
      "[[0.99936527 0.99926215 0.9993793  ... 0.9963213  0.9975904  0.9976841 ]]\n",
      "[[0.9993654  0.9992624  0.9993794  ... 0.9963218  0.99759066 0.9976845 ]]\n",
      "[[0.9993649  0.9992618  0.99937904 ... 0.9963199  0.99758947 0.99768317]]\n",
      "[[0.99936527 0.99926215 0.9993793  ... 0.99632096 0.9975902  0.9976839 ]]\n",
      "[[0.99936455 0.9992613  0.99937856 ... 0.996318   0.99758816 0.99768186]]\n",
      "[[0.99936527 0.99926215 0.9993793  ... 0.9963212  0.9975903  0.9976841 ]]\n",
      "[[0.99936527 0.99926215 0.9993793  ... 0.9963211  0.9975902  0.9976839 ]]\n",
      "[[0.99936503 0.9992619  0.99937904 ... 0.9963201  0.9975896  0.9976833 ]]\n",
      "[[0.99936503 0.9992619  0.99937904 ... 0.9963201  0.9975896  0.9976833 ]]\n",
      "[[0.99936503 0.99926203 0.99937916 ... 0.9963205  0.9975898  0.9976835 ]]\n",
      "[[0.9993649  0.9992618  0.9993789  ... 0.99631965 0.99758923 0.9976829 ]]\n",
      "[[0.999366   0.999263   0.99938    ... 0.9963242  0.9975924  0.99768615]]\n",
      "[[0.99936384 0.9992606  0.99937797 ... 0.99631554 0.99758637 0.9976802 ]]\n",
      "[[0.9993648 0.9992617 0.9993789 ... 0.9963194 0.9975891 0.9976828]]\n",
      "[[0.9993655  0.9992625  0.9993795  ... 0.99632215 0.997591   0.9976847 ]]\n",
      "[[0.9993655  0.9992624  0.9993795  ... 0.99632204 0.9975909  0.9976846 ]]\n",
      "[[0.9993647  0.99926156 0.9993788  ... 0.99631894 0.99758875 0.99768245]]\n",
      "[[0.99936503 0.9992619  0.99937904 ... 0.9963201  0.9975896  0.9976833 ]]\n",
      "[[0.99936503 0.9992619  0.99937904 ... 0.99632025 0.9975897  0.9976834 ]]\n",
      "[[0.9993655 0.9992625 0.9993795 ... 0.9963223 0.997591  0.9976847]]\n",
      "[[0.99936515 0.99926203 0.99937916 ... 0.9963207  0.99759007 0.99768376]]\n",
      "[[0.99936515 0.99926203 0.99937916 ... 0.9963207  0.99759007 0.99768376]]\n",
      "[[0.9993648  0.99926156 0.9993788  ... 0.99631906 0.99758875 0.9976826 ]]\n",
      "[[0.99936527 0.99926215 0.9993793  ... 0.9963211  0.9975903  0.997684  ]]\n",
      "[[0.99936444 0.9992612  0.99937844 ... 0.9963176  0.9975878  0.9976816 ]]\n",
      "[[0.99936455 0.9992613  0.99937856 ... 0.996318   0.99758804 0.99768186]]\n",
      "[[0.9993649  0.9992618  0.9993789  ... 0.9963198  0.99758935 0.99768305]]\n",
      "[[0.99936587 0.99926287 0.9993799  ... 0.9963236  0.997592   0.9976857 ]]\n",
      "[[0.99936527 0.99926215 0.9993793  ... 0.99632096 0.9975902  0.9976839 ]]\n",
      "[[0.9993654  0.9992623  0.9993794  ... 0.9963217  0.99759066 0.99768436]]\n",
      "[[0.99936444 0.9992612  0.99937856 ... 0.99631786 0.9975879  0.99768174]]\n",
      "[[0.99936527 0.9992623  0.9993794  ... 0.99632144 0.99759054 0.99768424]]\n",
      "[[0.9993654  0.9992623  0.9993794  ... 0.99632156 0.99759054 0.99768424]]\n",
      "[[0.9993654 0.9992624 0.9993795 ... 0.9963219 0.9975908 0.9976845]]\n",
      "[[0.9993642  0.99926096 0.9993783  ... 0.9963168  0.9975872  0.997681  ]]\n",
      "[[0.9993654 0.9992624 0.9993794 ... 0.9963219 0.9975908 0.9976845]]\n",
      "[[0.9993656  0.99926263 0.99937963 ... 0.99632263 0.99759126 0.99768496]]\n",
      "[[0.999366   0.999263   0.99938    ... 0.99632406 0.9975923  0.997686  ]]\n",
      "[[0.9993654 0.9992624 0.9993794 ... 0.9963219 0.9975908 0.9976845]]\n",
      "[[0.99936527 0.9992623  0.9993794  ... 0.99632144 0.9975904  0.9976841 ]]\n",
      "[[0.99936515 0.99926203 0.99937916 ... 0.9963205  0.9975898  0.9976835 ]]\n",
      "[[0.9993655  0.9992624  0.9993795  ... 0.99632204 0.9975909  0.9976846 ]]\n",
      "[[0.99936587 0.99926287 0.9993799  ... 0.9963238  0.9975921  0.9976858 ]]\n",
      "[[0.99936515 0.99926203 0.99937916 ... 0.9963206  0.99758995 0.99768364]]\n",
      "[[0.99936527 0.99926215 0.9993793  ... 0.9963213  0.9975904  0.9976841 ]]\n",
      "[[0.99936527 0.99926215 0.9993793  ... 0.99632096 0.9975902  0.9976839 ]]\n",
      "[[0.9993654  0.9992623  0.9993794  ... 0.9963217  0.99759066 0.99768436]]\n",
      "[[0.9993655  0.9992624  0.9993795  ... 0.99632204 0.9975909  0.9976846 ]]\n",
      "[[0.99936527 0.99926215 0.9993793  ... 0.9963211  0.9975903  0.997684  ]]\n",
      "[[0.99936527 0.9992623  0.9993794  ... 0.99632144 0.9975904  0.99768424]]\n",
      "[[0.99936515 0.99926203 0.99937916 ... 0.9963207  0.99758995 0.99768364]]\n",
      "[[0.99936527 0.99926215 0.9993793  ... 0.9963211  0.9975903  0.997684  ]]\n",
      "[[0.9993655  0.9992624  0.9993795  ... 0.99632204 0.9975909  0.9976846 ]]\n",
      "[[0.99936575 0.99926275 0.9993799  ... 0.99632347 0.99759185 0.99768555]]\n",
      "[[0.9993654 0.9992624 0.9993795 ... 0.9963219 0.9975908 0.9976846]]\n",
      "[[0.9993655  0.9992624  0.9993795  ... 0.99632215 0.997591   0.9976847 ]]\n",
      "[[0.9993642  0.99926096 0.9993783  ... 0.9963169  0.9975873  0.99768114]]\n",
      "[[0.99936503 0.9992619  0.99937904 ... 0.9963201  0.9975896  0.9976833 ]]\n",
      "[[0.99936515 0.99926203 0.99937916 ... 0.99632084 0.99759007 0.99768376]]\n",
      "[[0.9993655  0.9992624  0.9993795  ... 0.99632204 0.9975909  0.9976846 ]]\n",
      "[[0.9993655  0.9992625  0.9993795  ... 0.9963223  0.99759114 0.99768484]]\n",
      "[[0.99936527 0.9992623  0.9993794  ... 0.9963213  0.9975904  0.9976841 ]]\n",
      "[[0.9993655  0.9992624  0.9993795  ... 0.99632204 0.9975909  0.9976846 ]]\n",
      "[[0.99936515 0.99926203 0.99937916 ... 0.9963206  0.99758995 0.99768364]]\n",
      "[[0.9993654  0.9992623  0.9993794  ... 0.99632156 0.99759066 0.99768436]]\n",
      "[[0.99936587 0.99926287 0.9993799  ... 0.9963237  0.9975921  0.9976858 ]]\n",
      "[[0.9993649  0.9992618  0.99937904 ... 0.9963199  0.99758935 0.99768317]]\n",
      "[[0.9993654  0.9992623  0.9993794  ... 0.99632156 0.99759054 0.99768436]]\n",
      "[[0.9993655  0.9992624  0.9993795  ... 0.99632204 0.9975909  0.9976846 ]]\n",
      "[[0.9993656  0.99926263 0.99937963 ... 0.99632263 0.9975914  0.99768496]]\n",
      "[[0.9993654 0.9992624 0.9993794 ... 0.9963218 0.9975908 0.9976845]]\n",
      "[[0.9993655  0.9992624  0.9993795  ... 0.99632215 0.997591   0.9976847 ]]\n",
      "[[0.9993636  0.99926025 0.9993777  ... 0.9963141  0.9975854  0.9976793 ]]\n",
      "[[0.99936444 0.9992613  0.99937856 ... 0.99631786 0.99758804 0.99768174]]\n",
      "[[0.9993655 0.9992625 0.9993795 ... 0.9963223 0.997591  0.9976847]]\n",
      "[[0.9993649  0.9992618  0.9993789  ... 0.9963198  0.99758923 0.99768305]]\n",
      "[[0.9993647  0.99926144 0.9993787  ... 0.9963187  0.9975885  0.99768233]]\n",
      "[[0.9993654 0.9992624 0.9993794 ... 0.9963218 0.9975908 0.9976845]]\n",
      "[[0.9993661  0.9992631  0.9993801  ... 0.9963246  0.99759275 0.9976864 ]]\n",
      "[[0.9993649  0.9992618  0.9993789  ... 0.9963198  0.99758935 0.99768305]]\n",
      "[[0.9993649  0.9992618  0.99937904 ... 0.9963199  0.99758935 0.99768317]]\n",
      "[[0.99936503 0.99926203 0.99937916 ... 0.9963205  0.9975898  0.9976835 ]]\n",
      "[[0.9993656  0.99926263 0.99937963 ... 0.9963229  0.9975915  0.9976852 ]]\n",
      "[[0.99936396 0.9992607  0.9993781  ... 0.9963159  0.9975866  0.9976804 ]]\n",
      "[[0.9993649  0.9992618  0.99937904 ... 0.9963199  0.99758935 0.99768317]]\n",
      "[[0.9993649  0.9992618  0.99937904 ... 0.9963199  0.99758947 0.99768317]]\n",
      "[[0.9993655 0.9992625 0.9993795 ... 0.9963223 0.997591  0.9976847]]\n",
      "[[0.99936575 0.99926263 0.99937975 ... 0.996323   0.9975916  0.9976853 ]]\n",
      "[[0.99936515 0.99926203 0.99937916 ... 0.9963206  0.99758995 0.99768364]]\n",
      "[[0.99936527 0.99926215 0.9993793  ... 0.9963213  0.9975904  0.9976841 ]]\n",
      "[[0.9993656  0.9992625  0.99937963 ... 0.9963225  0.99759126 0.99768496]]\n",
      "[[0.99936515 0.99926203 0.99937916 ... 0.9963207  0.99758995 0.99768376]]\n",
      "[[0.99936444 0.9992613  0.99937856 ... 0.996318   0.99758804 0.99768186]]\n",
      "[[0.9993656  0.9992625  0.99937963 ... 0.9963225  0.99759126 0.99768496]]\n",
      "[[0.99936503 0.99926203 0.99937916 ... 0.9963205  0.9975898  0.9976835 ]]\n",
      "[[0.9993656  0.9992625  0.99937963 ... 0.9963225  0.99759126 0.99768496]]\n",
      "[[0.99936515 0.99926203 0.99937916 ... 0.9963207  0.99758995 0.99768364]]\n",
      "[[0.9993655  0.9992625  0.9993795  ... 0.9963224  0.99759114 0.99768484]]\n",
      "[[0.99936527 0.99926215 0.9993793  ... 0.9963212  0.9975903  0.9976841 ]]\n",
      "[[0.9993654  0.9992623  0.9993794  ... 0.9963217  0.99759066 0.99768436]]\n",
      "[[0.9993654  0.9992623  0.9993794  ... 0.99632156 0.99759054 0.99768436]]\n",
      "[[0.9993648 0.9992617 0.9993789 ... 0.9963194 0.997589  0.9976828]]\n",
      "[[0.99936503 0.9992619  0.99937904 ... 0.99632025 0.9975897  0.9976834 ]]\n",
      "[[0.99936503 0.9992619  0.99937904 ... 0.99632025 0.9975897  0.9976834 ]]\n",
      "[[0.9993656  0.99926263 0.99937963 ... 0.99632275 0.9975914  0.9976851 ]]\n",
      "[[0.9993654  0.9992623  0.9993794  ... 0.99632156 0.99759054 0.99768424]]\n",
      "[[0.99936527 0.9992623  0.9993794  ... 0.99632144 0.9975904  0.9976841 ]]\n",
      "[[0.9993637  0.9992605  0.99937785 ... 0.99631506 0.997586   0.9976799 ]]\n",
      "[[0.99936515 0.99926203 0.99937916 ... 0.9963207  0.99758995 0.99768364]]\n",
      "[[0.9993654  0.9992623  0.9993794  ... 0.99632156 0.99759054 0.99768436]]\n",
      "[[0.99936527 0.99926215 0.9993793  ... 0.9963212  0.9975903  0.997684  ]]\n",
      "[[0.9993649  0.9992618  0.9993789  ... 0.99631965 0.99758923 0.9976829 ]]\n",
      "[[0.9993656  0.99926263 0.99937963 ... 0.99632263 0.9975914  0.9976851 ]]\n",
      "[[0.99936503 0.9992619  0.99937916 ... 0.99632037 0.9975898  0.9976835 ]]\n",
      "[[0.9993649  0.9992617  0.9993789  ... 0.99631953 0.99758923 0.9976829 ]]\n",
      "[[0.9993649  0.9992618  0.99937904 ... 0.9963198  0.99758935 0.99768305]]\n",
      "[[0.99936527 0.99926215 0.9993793  ... 0.9963212  0.9975903  0.9976841 ]]\n",
      "[[0.99936455 0.99926144 0.9993787  ... 0.9963182  0.9975883  0.9976821 ]]\n",
      "[[0.9993649  0.9992618  0.9993789  ... 0.9963198  0.99758935 0.99768305]]\n",
      "[[0.9993643  0.9992611  0.99937844 ... 0.9963174  0.9975877  0.9976815 ]]\n",
      "[[0.99936503 0.9992619  0.99937904 ... 0.9963201  0.9975896  0.9976833 ]]\n",
      "[[0.9993654  0.9992623  0.9993794  ... 0.9963217  0.99759066 0.99768436]]\n",
      "[[0.9993647  0.99926156 0.9993788  ... 0.99631894 0.99758875 0.99768245]]\n",
      "[[0.9993649  0.9992618  0.99937904 ... 0.99632    0.99758947 0.99768317]]\n",
      "[[0.99936587 0.99926287 0.9993799  ... 0.9963237  0.9975921  0.9976857 ]]\n",
      "[[0.9993649  0.9992618  0.99937904 ... 0.9963199  0.99758935 0.99768317]]\n",
      "[[0.99936527 0.9992623  0.9993794  ... 0.9963213  0.9975904  0.9976841 ]]\n",
      "[[0.99936503 0.9992619  0.99937916 ... 0.99632037 0.9975897  0.9976834 ]]\n",
      "[[0.99936503 0.9992619  0.99937904 ... 0.9963201  0.9975896  0.9976833 ]]\n",
      "[[0.9993656  0.99926263 0.99937963 ... 0.99632263 0.9975914  0.9976851 ]]\n",
      "[[0.9993654  0.9992624  0.9993794  ... 0.9963218  0.99759066 0.99768436]]\n",
      "[[0.9993642  0.99926096 0.9993783  ... 0.9963169  0.9975873  0.99768114]]\n",
      "[[0.99936396 0.9992606  0.99937797 ... 0.99631566 0.99758637 0.9976802 ]]\n",
      "[[0.9993648  0.99926156 0.9993788  ... 0.99631906 0.99758875 0.9976826 ]]\n",
      "[[0.99936575 0.99926263 0.99937975 ... 0.996323   0.9975916  0.9976853 ]]\n",
      "[[0.99936575 0.99926275 0.99937975 ... 0.9963232  0.99759173 0.99768543]]\n",
      "[[0.9993643  0.9992611  0.9993783  ... 0.996317   0.99758744 0.99768126]]\n",
      "[[0.9993654  0.9992623  0.9993794  ... 0.9963217  0.99759066 0.99768436]]\n",
      "[[0.999366   0.999263   0.99938    ... 0.9963243  0.9975924  0.99768615]]\n",
      "[[0.9993654 0.9992624 0.9993795 ... 0.9963219 0.9975909 0.9976846]]\n",
      "[[0.9993655  0.9992625  0.99937963 ... 0.9963225  0.99759126 0.99768484]]\n",
      "[[0.99936515 0.99926203 0.99937916 ... 0.99632084 0.99759007 0.99768376]]\n",
      "[[0.9993656  0.99926263 0.99937975 ... 0.9963229  0.9975915  0.9976852 ]]\n",
      "[[0.99936575 0.99926275 0.9993799  ... 0.99632347 0.99759185 0.99768555]]\n",
      "[[0.9993654 0.9992624 0.9993794 ... 0.9963218 0.9975908 0.9976845]]\n",
      "[[0.99936515 0.99926203 0.99937916 ... 0.9963206  0.99758995 0.99768364]]\n",
      "[[0.99936396 0.9992607  0.9993781  ... 0.9963159  0.9975866  0.9976804 ]]\n",
      "[[0.99936503 0.9992619  0.99937916 ... 0.99632037 0.9975897  0.9976835 ]]\n",
      "[[0.9993656  0.99926263 0.99937963 ... 0.9963229  0.9975915  0.9976852 ]]\n",
      "[[0.9993656  0.99926263 0.99937963 ... 0.9963229  0.9975915  0.9976852 ]]\n",
      "[[0.9993654  0.9992623  0.9993794  ... 0.9963217  0.99759066 0.99768436]]\n",
      "[[0.9993656  0.9992625  0.99937963 ... 0.9963225  0.99759126 0.99768496]]\n",
      "[[0.9993649  0.9992618  0.9993789  ... 0.9963198  0.99758935 0.99768305]]\n",
      "[[0.9993649  0.9992617  0.9993789  ... 0.99631953 0.9975891  0.9976829 ]]\n",
      "[[0.99936575 0.99926275 0.99937975 ... 0.99632335 0.99759173 0.99768543]]\n",
      "[[0.9993654  0.9992623  0.9993794  ... 0.9963217  0.99759066 0.99768436]]\n",
      "[[0.9993654 0.9992624 0.9993795 ... 0.9963219 0.9975908 0.9976845]]\n",
      "[[0.99936503 0.9992619  0.99937904 ... 0.9963201  0.9975896  0.9976834 ]]\n",
      "[[0.99936575 0.99926263 0.99937975 ... 0.996323   0.9975916  0.9976852 ]]\n",
      "[[0.9993656  0.9992625  0.99937963 ... 0.99632263 0.99759126 0.99768496]]\n",
      "[[0.9993649  0.9992618  0.9993789  ... 0.99631965 0.99758923 0.9976829 ]]\n",
      "[[0.99936515 0.99926203 0.99937916 ... 0.99632084 0.99759007 0.99768376]]\n",
      "[[0.99936515 0.99926203 0.99937916 ... 0.9963207  0.99758995 0.99768364]]\n",
      "[[0.99936503 0.9992619  0.99937916 ... 0.99632037 0.9975897  0.9976835 ]]\n",
      "[[0.99936515 0.99926203 0.99937916 ... 0.99632084 0.99759007 0.99768376]]\n",
      "[[0.99936503 0.9992619  0.99937916 ... 0.99632037 0.9975898  0.9976835 ]]\n",
      "[[0.9993655  0.9992624  0.9993795  ... 0.99632215 0.997591   0.9976847 ]]\n",
      "[[0.9993648 0.9992617 0.9993788 ... 0.9963192 0.9975889 0.9976827]]\n",
      "[[0.9993647  0.99926156 0.9993787  ... 0.9963187  0.99758863 0.99768233]]\n",
      "[[0.9993649  0.9992618  0.99937904 ... 0.9963199  0.99758947 0.99768317]]\n",
      "[[0.99936587 0.99926287 0.9993799  ... 0.9963236  0.997592   0.9976857 ]]\n",
      "[[0.9993655  0.9992625  0.9993795  ... 0.9963224  0.99759114 0.99768484]]\n",
      "[[0.9993655  0.9992625  0.99937963 ... 0.9963224  0.99759114 0.99768484]]\n",
      "[[0.9993649  0.9992618  0.9993789  ... 0.99631965 0.99758923 0.99768305]]\n",
      "[[0.99936503 0.9992619  0.99937904 ... 0.99632025 0.9975896  0.9976834 ]]\n",
      "[[0.99936527 0.9992623  0.9993794  ... 0.99632144 0.9975904  0.99768424]]\n",
      "[[0.99936527 0.99926215 0.9993793  ... 0.9963211  0.9975902  0.997684  ]]\n",
      "[[0.99936527 0.99926215 0.9993793  ... 0.9963211  0.9975903  0.997684  ]]\n",
      "[[0.99936587 0.99926287 0.9993799  ... 0.9963236  0.997592   0.9976857 ]]\n",
      "[[0.99936455 0.9992613  0.99937856 ... 0.9963181  0.99758816 0.997682  ]]\n",
      "[[0.99936587 0.99926287 0.9993799  ... 0.9963237  0.9975921  0.9976858 ]]\n",
      "[[0.99936515 0.99926203 0.99937916 ... 0.9963206  0.99758995 0.99768364]]\n",
      "[[0.9993654  0.9992623  0.9993794  ... 0.99632156 0.99759054 0.99768424]]\n",
      "[[0.99936527 0.99926215 0.9993793  ... 0.9963211  0.9975902  0.997684  ]]\n",
      "[[0.99936587 0.99926287 0.9993799  ... 0.9963237  0.9975921  0.9976858 ]]\n",
      "[[0.9993648 0.9992617 0.9993788 ... 0.9963192 0.9975889 0.9976827]]\n",
      "[[0.99936575 0.99926275 0.99937975 ... 0.9963231  0.9975916  0.9976853 ]]\n",
      "[[0.99936515 0.99926203 0.99937916 ... 0.9963205  0.9975898  0.9976835 ]]\n",
      "[[0.99936503 0.9992619  0.99937904 ... 0.99632    0.99758947 0.9976833 ]]\n",
      "[[0.99936455 0.9992613  0.9993787  ... 0.9963182  0.9975883  0.997682  ]]\n",
      "[[0.999366  0.999263  0.99938   ... 0.9963242 0.9975923 0.997686 ]]\n",
      "[[0.99936515 0.99926203 0.99937916 ... 0.9963205  0.9975898  0.9976835 ]]\n",
      "[[0.9993642  0.9992611  0.9993783  ... 0.996317   0.9975873  0.99768114]]\n",
      "[[0.9993654  0.9992623  0.9993794  ... 0.9963217  0.99759066 0.99768436]]\n",
      "[[0.9993649  0.9992618  0.99937904 ... 0.99632    0.99758947 0.99768317]]\n",
      "[[0.99936515 0.99926203 0.99937916 ... 0.9963207  0.99758995 0.99768364]]\n",
      "[[0.99936455 0.9992613  0.99937856 ... 0.9963181  0.99758816 0.997682  ]]\n",
      "[[0.99936503 0.99926203 0.99937916 ... 0.9963205  0.9975898  0.9976835 ]]\n",
      "[[0.9993655  0.9992625  0.99937963 ... 0.9963225  0.99759126 0.99768496]]\n",
      "[[0.9993648  0.99926156 0.9993788  ... 0.99631906 0.9975889  0.9976826 ]]\n",
      "[[0.9993649  0.9992618  0.9993789  ... 0.99631965 0.99758923 0.99768305]]\n",
      "[[0.9993656  0.99926263 0.99937963 ... 0.99632275 0.9975914  0.9976851 ]]\n",
      "[[0.99936444 0.9992612  0.99937856 ... 0.99631774 0.9975879  0.99768174]]\n",
      "[[0.9993655  0.9992625  0.9993795  ... 0.99632215 0.997591   0.9976847 ]]\n",
      "[[0.9993648 0.9992617 0.9993789 ... 0.9963193 0.997589  0.9976828]]\n",
      "[[0.99936515 0.99926203 0.9993793  ... 0.99632084 0.99759007 0.99768376]]\n",
      "[[0.9993656  0.9992625  0.99937963 ... 0.99632263 0.99759126 0.99768496]]\n",
      "[[0.9993654 0.9992624 0.9993795 ... 0.9963219 0.9975908 0.9976845]]\n",
      "[[0.9993654  0.9992623  0.9993794  ... 0.9963217  0.99759066 0.99768436]]\n",
      "[[0.9993655  0.9992625  0.9993795  ... 0.9963224  0.99759114 0.99768484]]\n",
      "[[0.99936503 0.9992619  0.99937904 ... 0.99632    0.99758947 0.9976833 ]]\n",
      "[[0.99936575 0.99926263 0.99937975 ... 0.996323   0.9975916  0.9976853 ]]\n",
      "[[0.9993654  0.9992623  0.9993794  ... 0.99632156 0.99759054 0.99768424]]\n",
      "[[0.99936503 0.9992619  0.99937904 ... 0.9963201  0.9975896  0.9976833 ]]\n",
      "[[0.99936527 0.99926215 0.9993793  ... 0.9963211  0.9975903  0.997684  ]]\n",
      "[[0.9993648 0.9992617 0.9993788 ... 0.9963193 0.997589  0.9976827]]\n",
      "[[0.9993654  0.9992623  0.9993794  ... 0.9963217  0.99759066 0.99768436]]\n",
      "[[0.99936455 0.9992613  0.99937856 ... 0.9963181  0.99758816 0.997682  ]]\n",
      "[[0.9993654  0.9992623  0.9993794  ... 0.9963217  0.99759066 0.99768436]]\n",
      "[[0.9993654 0.9992624 0.9993794 ... 0.9963218 0.9975908 0.9976845]]\n",
      "[[0.99936587 0.99926287 0.9993799  ... 0.9963238  0.9975922  0.9976858 ]]\n",
      "[[0.9993643  0.9992612  0.99937844 ... 0.9963175  0.9975877  0.9976815 ]]\n",
      "[[0.99936515 0.99926215 0.9993793  ... 0.99632096 0.9975902  0.9976839 ]]\n",
      "[[0.99936575 0.99926275 0.99937975 ... 0.9963231  0.9975916  0.9976853 ]]\n",
      "[[0.99936444 0.9992612  0.99937856 ... 0.99631774 0.9975879  0.99768174]]\n",
      "[[0.9993654 0.9992624 0.9993795 ... 0.9963219 0.9975908 0.9976845]]\n",
      "[[0.99936527 0.99926215 0.9993793  ... 0.9963212  0.9975903  0.997684  ]]\n",
      "[[0.9993636  0.99926025 0.9993777  ... 0.99631435 0.99758554 0.9976794 ]]\n",
      "[[0.9993642  0.99926096 0.9993783  ... 0.9963169  0.9975873  0.99768114]]\n",
      "[[0.9993654 0.9992624 0.9993794 ... 0.9963218 0.9975908 0.9976845]]\n",
      "[[0.99936336 0.99926    0.99937755 ... 0.9963133  0.9975847  0.9976787 ]]\n",
      "[[0.9993654  0.9992623  0.9993794  ... 0.9963217  0.99759066 0.99768436]]\n",
      "[[0.99936575 0.99926275 0.99937975 ... 0.99632335 0.99759185 0.99768555]]\n",
      "[[0.99936515 0.99926203 0.99937916 ... 0.9963207  0.99759007 0.99768376]]\n",
      "[[0.99936503 0.9992619  0.99937904 ... 0.9963201  0.9975896  0.9976833 ]]\n",
      "[[0.99936515 0.99926203 0.99937916 ... 0.9963206  0.99758995 0.99768364]]\n",
      "[[0.9993656  0.99926263 0.99937963 ... 0.99632275 0.9975914  0.9976851 ]]\n",
      "[[0.99936503 0.9992619  0.99937904 ... 0.9963201  0.9975896  0.9976833 ]]\n",
      "[[0.9993649  0.9992618  0.99937904 ... 0.9963199  0.99758947 0.99768317]]\n",
      "[[0.99936527 0.9992623  0.9993793  ... 0.9963213  0.9975904  0.9976841 ]]\n",
      "[[0.9993655  0.9992625  0.9993795  ... 0.9963224  0.99759114 0.99768484]]\n",
      "[[0.99936515 0.99926203 0.99937916 ... 0.9963206  0.99758995 0.99768364]]\n",
      "[[0.99936575 0.99926275 0.99937975 ... 0.9963232  0.99759173 0.99768543]]\n",
      "[[0.9993656  0.99926263 0.99937963 ... 0.99632275 0.9975914  0.9976851 ]]\n",
      "[[0.9993655 0.9992625 0.9993795 ... 0.9963223 0.997591  0.9976847]]\n",
      "[[0.99936503 0.9992619  0.99937904 ... 0.9963201  0.9975896  0.9976833 ]]\n",
      "[[0.99936455 0.9992613  0.99937856 ... 0.9963182  0.9975883  0.997682  ]]\n",
      "[[0.99936527 0.99926215 0.9993793  ... 0.9963213  0.9975904  0.9976841 ]]\n",
      "[[0.99936515 0.99926203 0.99937916 ... 0.9963205  0.9975898  0.99768364]]\n",
      "[[0.99936587 0.999263   0.9993799  ... 0.99632394 0.9975922  0.9976859 ]]\n",
      "[[0.99936515 0.99926203 0.99937916 ... 0.99632084 0.99759007 0.99768376]]\n",
      "[[0.9993649  0.9992618  0.99937904 ... 0.9963199  0.99758935 0.99768317]]\n",
      "[[0.99936527 0.9992623  0.9993794  ... 0.9963213  0.9975904  0.9976841 ]]\n",
      "[[0.9993656  0.9992625  0.99937963 ... 0.9963225  0.99759126 0.99768496]]\n",
      "[[0.99936527 0.9992623  0.9993794  ... 0.9963213  0.9975904  0.9976841 ]]\n",
      "[[0.9993656  0.99926263 0.99937963 ... 0.99632275 0.9975914  0.9976851 ]]\n",
      "[[0.9993654  0.9992624  0.9993794  ... 0.9963218  0.99759066 0.9976845 ]]\n",
      "[[0.99936455 0.9992613  0.99937856 ... 0.9963181  0.99758816 0.997682  ]]\n",
      "[[0.9993654  0.9992623  0.9993794  ... 0.99632144 0.99759054 0.99768424]]\n",
      "[[0.99936515 0.99926215 0.9993793  ... 0.99632096 0.9975902  0.9976839 ]]\n",
      "[[0.9993654  0.9992623  0.9993794  ... 0.9963217  0.99759066 0.99768436]]\n",
      "[[0.9993635  0.9992601  0.99937767 ... 0.996314   0.9975852  0.9976792 ]]\n",
      "[[0.99936587 0.99926287 0.9993799  ... 0.99632347 0.997592   0.99768555]]\n",
      "[[0.9993654  0.9992623  0.9993794  ... 0.99632156 0.99759054 0.99768424]]\n",
      "[[0.99936527 0.99926215 0.9993793  ... 0.9963213  0.9975904  0.9976841 ]]\n",
      "[[0.99936444 0.9992612  0.99937844 ... 0.9963176  0.9975878  0.9976816 ]]\n",
      "[[0.9993637  0.99926037 0.99937785 ... 0.9963148  0.9975858  0.99767965]]\n",
      "[[0.99936575 0.99926275 0.99937975 ... 0.99632347 0.99759185 0.99768555]]\n",
      "[[0.9993649  0.9992618  0.99937904 ... 0.99632    0.99758947 0.99768317]]\n",
      "[[0.9993642  0.99926096 0.9993783  ... 0.9963168  0.9975872  0.997681  ]]\n",
      "[[0.9993655  0.9992625  0.99937963 ... 0.9963224  0.99759114 0.99768484]]\n",
      "[[0.99936515 0.99926203 0.99937916 ... 0.9963207  0.99758995 0.99768364]]\n",
      "[[0.99936527 0.99926215 0.9993793  ... 0.9963213  0.9975904  0.9976841 ]]\n",
      "[[0.9993655  0.9992624  0.9993795  ... 0.99632215 0.9975909  0.9976846 ]]\n",
      "[[0.99936515 0.99926203 0.99937916 ... 0.9963206  0.9975898  0.99768364]]\n",
      "[[0.99936587 0.999263   0.9993799  ... 0.99632394 0.9975922  0.9976859 ]]\n",
      "[[0.9993655  0.9992624  0.9993795  ... 0.99632204 0.9975909  0.9976846 ]]\n",
      "[[0.99936575 0.99926263 0.99937975 ... 0.996323   0.9975916  0.9976853 ]]\n",
      "[[0.9993654 0.9992624 0.9993795 ... 0.9963219 0.9975908 0.9976845]]\n",
      "[[0.9993654 0.9992624 0.9993794 ... 0.9963218 0.9975908 0.9976845]]\n",
      "[[0.99936527 0.99926215 0.9993793  ... 0.9963212  0.9975903  0.9976841 ]]\n",
      "[[0.9993654  0.9992623  0.9993794  ... 0.99632156 0.99759054 0.99768424]]\n",
      "[[0.99936527 0.9992623  0.9993794  ... 0.99632144 0.99759054 0.99768424]]\n",
      "[[0.9993655  0.9992625  0.9993795  ... 0.9963223  0.997591   0.99768484]]\n",
      "[[0.9993649  0.9992618  0.99937904 ... 0.9963199  0.99758947 0.99768317]]\n",
      "[[0.99936396 0.9992607  0.9993781  ... 0.996316   0.9975866  0.9976804 ]]\n",
      "[[0.99936527 0.99926215 0.9993793  ... 0.9963213  0.9975904  0.9976841 ]]\n",
      "[[0.9993648 0.9992617 0.9993789 ... 0.9963194 0.997589  0.9976828]]\n",
      "[[0.99936503 0.9992619  0.99937904 ... 0.9963201  0.9975896  0.9976833 ]]\n",
      "[[0.9993641  0.99926084 0.9993782  ... 0.99631643 0.9975871  0.9976808 ]]\n",
      "[[0.9993654 0.9992624 0.9993794 ... 0.9963218 0.9975908 0.9976845]]\n",
      "[[0.99936503 0.9992619  0.99937904 ... 0.9963201  0.9975896  0.9976833 ]]\n",
      "[[0.99936515 0.99926203 0.99937916 ... 0.9963207  0.99758995 0.99768364]]\n",
      "[[0.99936455 0.9992613  0.99937856 ... 0.9963181  0.99758816 0.99768186]]\n",
      "[[0.99936527 0.99926215 0.9993793  ... 0.9963212  0.9975903  0.9976841 ]]\n",
      "[[0.99936503 0.99926203 0.99937916 ... 0.9963205  0.9975898  0.9976835 ]]\n",
      "[[0.9993654  0.9992623  0.9993794  ... 0.99632156 0.99759054 0.99768436]]\n",
      "[[0.9993647  0.99926144 0.9993787  ... 0.99631846 0.9975884  0.9976822 ]]\n",
      "[[0.9993647  0.99926144 0.9993787  ... 0.9963187  0.9975885  0.99768233]]\n",
      "[[0.9993647  0.99926156 0.9993788  ... 0.9963188  0.99758863 0.99768233]]\n",
      "[[0.99936455 0.99926144 0.9993787  ... 0.99631834 0.9975883  0.9976821 ]]\n",
      "[[0.99936503 0.9992619  0.99937916 ... 0.99632037 0.9975898  0.9976835 ]]\n",
      "[[0.9993642  0.99926096 0.9993783  ... 0.9963169  0.9975873  0.99768114]]\n",
      "[[0.9993649  0.9992618  0.99937904 ... 0.9963199  0.99758935 0.99768317]]\n",
      "[[0.9993654 0.9992624 0.9993794 ... 0.9963218 0.9975908 0.9976845]]\n",
      "[[0.99936336 0.99926    0.99937755 ... 0.9963133  0.9975848  0.9976787 ]]\n",
      "[[0.9993654  0.9992623  0.9993794  ... 0.9963217  0.99759066 0.99768436]]\n",
      "[[0.99936527 0.9992623  0.9993794  ... 0.99632144 0.99759054 0.99768424]]\n",
      "[[0.9993656  0.99926263 0.99937975 ... 0.996323   0.9975915  0.9976852 ]]\n",
      "[[0.9993648 0.9992617 0.9993789 ... 0.9963193 0.997589  0.9976828]]\n",
      "[[0.99936455 0.99926144 0.9993787  ... 0.9963182  0.9975883  0.9976821 ]]\n",
      "[[0.9993649  0.9992618  0.9993789  ... 0.9963198  0.99758935 0.99768305]]\n"
     ]
    }
   ],
   "source": [
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "video = cv2.VideoWriter(\"out.mp4\", fourcc, 30.0, (208, 120))\n",
    "lv_in = X[0]\n",
    "\n",
    "for i in range(500):\n",
    "    temp = model.predict(lv_in.reshape(1, 24960))\n",
    "    print(temp)\n",
    "    img = temp\n",
    "    img = np.array(img).reshape(120,208,1)\n",
    "    img = img * 255\n",
    "    img = np.array(img).astype(\"uint8\")\n",
    "    img = cv2.cvtColor(img,cv2.COLOR_GRAY2RGB)\n",
    "    lv_in = temp\n",
    "    video.write(img)\n",
    "video.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp = np.random.random((1, 24960))\n",
    "print(temp)\n",
    "for layer in model.layers:\n",
    "    weights = layer.get_weights()\n",
    "    for weight in weights:\n",
    "        if weight.shape == (512,):\n",
    "            continue\n",
    "        print(temp.shape, weight.shape)\n",
    "        temp = np.dot(temp, weight)\n",
    "        print(temp)\n",
    "    print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp = model.predict(np.random.random((1, 24960)))\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.predict(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.predict(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "argmax_y = K.max(K.reshape(logits_y, (-1, N, M)), axis=-1, keepdims=True)\n",
    "argmax_y = K.equal(K.reshape(logits_y, (-1, N, M)), argmax_y)\n",
    "encoder = K.function([x], [argmax_y, x_hat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "code, x_hat_test = encoder([X_test[:100]])\n",
    "plt.imshow(X_test[1].reshape(120, 208), cmap='gray')\n",
    "plt.show()\n",
    "plt.imshow(code[1].reshape(N, M), cmap='gray')\n",
    "plt.show()\n",
    "plt.imshow(x_hat_test[1].reshape(120, 208), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_hat_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
